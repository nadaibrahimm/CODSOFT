import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import csv

data = pd.read_csv('/content/train_data.csv', on_bad_lines='skip')
with open('/content/train_data.csv', 'r') as file:
    for _ in range(5):
        print(file.readline())

data.head()
data.describe()
data.info()
data.isnull().sum()
data.dropna(inplace=True)
data.columns()
pip install nltk
import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt')

data = pd.read_csv('/content/train_data.csv', on_bad_lines='skip')
data['plot_summary'] = data.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)
data['tokenized_plot_summary'] = data['plot_summary'].apply(lambda x: word_tokenize(x) if pd.notnull(x) else [])
data['tokenized_plot_summary'] = data['tokenized_plot_summary'].apply(lambda x: ' '.join(x))
data.to_csv('tokenized_movies_data.csv', index=False)
pip install pyLDAvis
np.random.seed(42)
import seaborn as sns
import matplotlib.pyplot as plt
import re
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel
# Plotting tools
pip install pyLDAvis
import pyLDAvis
import pyLDAvis.gensim
# set options for rendering plots
%matplotlib inline
# display multiple outputs within a cell
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all";
import warnings
warnings.filterwarnings('ignore');
train = pd.read_csv("/content/tokenized_movies_data.csv")
test = pd.read_csv("/content/tokenized_test_movies_data.csv")
import pandas as pd

train = pd.read_csv('/content/tokenized_movies_data.csv', on_bad_lines='skip')
print(train.head())
print(train.shape)
print(train["plot_summary"].value_counts())
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

train = pd.read_csv('/content/tokenized_movies_data.csv', on_bad_lines='skip')
train['plot_summary'] = train['plot_summary'].str.replace(r'[^a-zA-Z0-9\s]', '', regex=True)
top_summaries = train['plot_summary'].value_counts().nlargest(10).index
filtered_data = train[train['plot_summary'].isin(top_summaries)]

plt.figure(figsize=(10, 6))
sns.countplot(x='plot_summary', data=filtered_data, palette='dark')
plt.xticks(rotation=90)
plt.show()
train = pd.read_csv('/content/tokenized_movies_data.csv', on_bad_lines='skip')
print(train.iloc[0])  # Display the first row to check the available columns
train = pd.read_csv('/content/tokenized_movies_data.csv', on_bad_lines='skip')
print(train.columns)
train['word_length'] = train.iloc[:, 0].apply(lambda x: len(str(x).split()))
print(train[train["plot_summary"] == "EAP"]["word_length"].describe())
# Load your data
train = pd.DataFrame({
    'plot_summary': ['Sample plot summary 1', 'Sample plot summary 2'],
    'tokenized_plot_summary': [['word1', 'word2'], ['word3', 'word4']],
    'word_length': [100, 150],
    'author': ['EAP', 'Other']
})
try:
    eap_word_length_desc = train[train["author"] == "EAP"]["word_length"].describe()
except KeyError as e:
    print(f"KeyError: {e}")
try:
    print(train[train["author"] == "EAP"]["plot_summary"].iloc[0])
    print(train[train["author"] == "EAP"]["tokenized_plot_summary"].iloc[0])
except KeyError as e:
    print(f"KeyError: {e}")
try:
    plt.hist(train[train["author"] == "EAP"]["word_length"], bins=10)
    plt.title("Word Length Distribution for EAP")
    plt.xlabel("Word Length")
    plt.ylabel("Frequency")
    plt.show()
except KeyError as e:
plt.figure(figsize=(10,5))
sns.set_context('talk')
sns.distplot(train['word_length'], color='black');
train = pd.DataFrame({
    'plot_summary': ['Oscar et la dame rose (2009)', 'Bada$$ Mothaf**kas (2013)', 'Frankenstein by Mary Shelley'],
    'tokenized_plot_summary': [['Oscar', 'lady', 'pink'], ['Bada$$', 'fists', 'fly'], ['Frankenstein', 'monster', 'creation']],
    'word_length': [150, 120, 170],
    'author': ['EAP', 'HPL', 'MWS']
})
eap_word_length_desc = train[train["author"] == "EAP"]["word_length"].describe()
print(eap_word_length_desc)
print("Edgar Allen Poe Plot Summary: ", train[train["author"] == "EAP"]["plot_summary"].iloc[0])
print("Edgar Allen Poe Tokenized Plot Summary: ", train[train["author"] == "EAP"]["tokenized_plot_summary"].iloc[0])
f, ax = plt.subplots(1, 3, figsize=(16, 6))
sns.histplot(train[train['author'] == 'EAP']['word_length'], ax=ax[0], kde=True)
ax[0].set_title('Edgar Allen Poe')
sns.histplot(train[train['author'] == 'HPL']['word_length'], ax=ax[1], color='r', kde=True)
ax[1].set_title('H.P. Lovecraft')
sns.histplot(train[train['author'] == 'MWS']['word_length'], ax=ax[2], color='g', kde=True)
ax[2].set_title('Mary Shelley')
plt.tight_layout()
plt.show()
train[train["author"] == "HPL"]["word_length"].describe()
train[train["author"] == "MWS"]["word_length"].describe()
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk
nltk.download('stopwords')
nltk.download('punkt')
data = train["plot_summary"]
stop_words = set(stopwords.words('english'))
word_tokens = word_tokenize(data[1])
filtered_sentence = [w for w in word_tokens if not w in stop_words]
print(word_tokens)
print(filtered_sentence)
def sent_to_words(sentences):
    for sentence in sentences:
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations

data_words = list(sent_to_words(data))

print(data_words[:1])
# flatten list and join together as a string
flat_list = [item for sublist in data_words for item in sublist]
str1 = ' '.join(flat_list)
print(str1)
bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100)
trigram = gensim.models.Phrases(bigram[data_words], threshold=100)
bigram_mod = gensim.models.phrases.Phraser(bigram)
trigram_mod = gensim.models.phrases.Phraser(trigram)
print(trigram_mod[bigram_mod[data_words[0]]])
bigrams = []
for phrase in bigram[data_words[:100]]:
    bigrams.append(phrase)
bigrams[:10]
from nltk.corpus import stopwords
from gensim.utils import simple_preprocess
stop_words = stopwords.words('english')
def remove_stopwords(texts):
    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]
train_text = remove_stopwords(train['plot_summary'])
test_text = remove_stopwords(test['plot_summary'])
train_text = [' '.join(sent) for sent in train_text]
test_text = [' '.join(sent) for sent in test_text]
from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer(analyzer = "word",   \
                             tokenizer = None,    \
                             preprocessor = None, \
                             stop_words = None,   \
                             max_features = 6000)

feature_vec = vectorizer.fit_transform(train_text)
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS
from wordcloud import ImageColorGenerator

# Sample DataFrame (create or load your actual DataFrame here)
data = {
    'text': [
        "This is a great product. I really enjoyed it!",
        "Not worth the money. I expected better quality.",
        "Fantastic! Will definitely buy again.",
        "It's okay, but I wouldn't recommend it to friends.",
        "Loved it! Perfect for what I needed."
        # Add more reviews as needed...
    ]
}
train_val = pd.DataFrame(data)

# Word Cloud Function
def plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0, 16.0),
                   title=None, title_size=40, image_color=False):
    stopwords = set(STOPWORDS)
    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}
    stopwords = stopwords.union(more_stopwords)

    wordcloud = WordCloud(background_color='black',
                          stopwords=stopwords,
                          max_words=max_words,
                          max_font_size=max_font_size,
                          random_state=42,
                          width=800,
                          height=400,
                          mask=mask)
    wordcloud.generate(str(text))

    plt.figure(figsize=figure_size)
    if image_color:
        image_colors = ImageColorGenerator(mask)
        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation="bilinear")
        plt.title(title, fontdict={'size': title_size,
                                    'verticalalignment': 'bottom'})
    else:
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.title(title, fontdict={'size': title_size, 'color': 'black',
                                    'verticalalignment': 'bottom'})
    plt.axis('off')
    plt.tight_layout()

# Generate Word Cloud
plot_wordcloud(train_val["text"], title="Word Cloud of Reviews")
plt.show()  # Show the plot
text_transformer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), lowercase=True, max_features=150000)
%%time
X_train_text = text_transformer.fit_transform(train_val['text'])
X_test_text = text_transformer.transform(test['text'])
X_train_text.shape, X_test_text.shape
pip install scikit-learn
from sklearn.linear_model import LogisticRegression
logit = LogisticRegression(C=5e1, solver='lbfgs', multi_class='multinomial', random_state=17, n_jobs=4)
from sklearn.model_selection import StratifiedKFold
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=17)
%%time
logit.fit(X_train_text, train_val['text'])
# Uninstall existing versions of sklearn and scikit-learn
!pip uninstall -y sklearn scikit-learn

# Install the specific versions of scikit-learn and eli5
!pip install scikit-learn==0.24.1
!pip install eli5==0.11.0

# Verify the installation
try:
    import eli5
    import sklearn

    print('ELI5 version:', eli5.__version__)
    print('Scikit-learn version:', sklearn.__version__)
except ImportError as e:
    print(f"Import error: {e}")
pip install eli5
eli5.show_weights(estimator=logit,
                  feature_names= list(text_transformer.get_feature_names()),
                 top=(50, 5))
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

# Example training data
X_train_text = [
    "This is the first document.",
    "This document is the second document.",
    "And this is the third one.",
    "Is this the first document?"
]
y_train = [0, 1, 1, 0]  # Example target variable

# Create and fit the vectorizer on the training data
vectorizer = TfidfVectorizer()
X_train_transformed = vectorizer.fit_transform(X_train_text)

# Train the Logistic Regression model
logit = LogisticRegression()
logit.fit(X_train_transformed, y_train)

# Example test data
X_test_text = [
    "This is a new document.",
    "This document is similar to the first."
]

# Transform test data using the same fitted vectorizer
X_test_transformed = vectorizer.transform(X_test_text)

# Now make predictions with the model
test_preds = logit.predict(X_test_transformed)

# Output the predictions
print(test_preds)  # This will print the predicted classes for the test data
pd.DataFrame(test_preds, columns=['label']).head()
pd.DataFrame(test_preds, columns=['label']).to_csv('logit_tf_idf_starter_submission.csv',
                                                  index_label='id')

    print(f"KeyError: {e}")

